<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yizhi Song</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yizhi Song
                </p>
                <p>I obtained my PhD in Computer Science from <a href="https://www.cs.purdue.edu/cgvlab/www/">CGVLab</a> at Purdue University, advised by Prof. <a href="https://www.cs.purdue.edu/homes/aliaga/">Daniel Aliaga</a>. Before coming to Purdue, I recieved my B.S. in Computer Science from Zhejiang University. I interned at <a href="https://www.qualcomm.com/">Qualcomm</a> in summer 2021, and worked as a research intern at <a href="https://research.adobe.com/">Adobe</a> in summer 2022 & 2023 & 2024. During my internships, I'm fortunate to work with Dr. <a href="https://menglin-wu.github.io/">Meng-Lin Wu</a>, Dr. <a href="https://zzutk.github.io/">Zhifei Zhang</a>, Dr. <a href="https://wxiong.me/">Wei Xiong</a> and Dr. <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>.
                </p>
                <p>
                  I'm interested in GenAI topics, including building diffusion models for image and video generation (especially on identity preservation), and designing MLLMs for video understanding.
                </p>
                <p>
                  I'm always open to research collaborations. Please feel free to reach out.
                </p>
                <p style="text-align:center">
                  <a href="mailto:songcx2211@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/yizhi_cv_202507.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=IUj5R3EAAAAJ&hl=en&oi=ao">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/song630">Github</a> &nbsp;/&nbsp;
                  <a href="https://linkedin.com/in/yizhi-david-song-1781311a9/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile-photo-2025.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>


<!-- News -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <h2>News</h2>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="0"><tbody>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 05-2025 </td>
    <td> We release <a href="https://hanghuacs.github.io/MMIG-Bench/">MMIG-Bench</a>, a comprehensive benchmark for evaluating multi-modal image generation models. </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 04-2025 </td>
    <td> Relocated to Seattle and started my job in Amazon Ring AI. </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 04-2025 </td>
    <td> <a href="https://arxiv.org/abs/2212.00932"> ObjectStitch </a> and <a href="https://song630.github.io/IMPRINT-Project-Page/"> IMPRINT </a> have been productized and appear in <a href="https://www.youtube.com/watch?v=HVxv5YdbdDg"><span style="color: red;"><u>Adobe Summit Sneak</u></span></a> and are mentioned in <a href="https://research.adobe.com/news/the-research-behind-project-vision-cast/"><span style="color: red;"><u>Adobe Research news</u></span></a>! </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 04-2025 </td>
    <td> <a href="https://arking1995.github.io/Kubrick/"> Kubrick </a> (video generation agent) has been accepted to <span style="color: red;">CVPR 2025 AI4CC Workshop</span>. </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 03-2025 </td>
    <td> Passed my PhD thesis defense! </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 01-2025 </td>
    <td> <a href="https://song630.github.io/Refine-by-Align-Project-Page/"> Refine-by-Align </a> (A model to refine/fix generative artifacts from any generated images) has been accepted to <span style="color: red;">ICLR 2025 </span>. </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 09-2024 </td>
    <td> We release <a href="https://groundingbooth.github.io/"> GroundingBooth </a>, an image customization model with finegrained layout control. </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 08-2024 </td>
    <td> We release <a href="https://arking1995.github.io/Kubrick/"> Kubrick </a>, the first multimodal agent-based video generation pipeline. </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 07-2024 </td>
    <td> <a href="https://arxiv.org/abs/2409.04559"> Thinking Outside the BBox </a> (a generative model that automatically drops & harmonizes foreground objects in background images at reasonable locations) has been accepted to <span style="color: red;">ECCV 2024 </span>. </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:10%;vertical-align:middle"> 02-2024 </td>
    <td> <a href="https://song630.github.io/IMPRINT-Project-Page/"> IMPRINT </a> (the state-of-the-art model for image customization / object dropping) has been accepted to <span style="color: red;">CVPR 2024 </span>. </td>
  </tr>
</tbody></table>
<br>
<br>
<br>

          
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <h2>Selected Research Work</h2>
      <!-- <p>
        details...
      </p> -->
    </td>
  </tr>
</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


<table
class="special-table"
style="width:100%;height:240px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>

    <!-- MMIG-Bench -->
    <tr>
      <td class="moving-cell" style="padding:20px;padding-top:0;padding-bottom:0;width:25%;vertical-align:middle">
        <a href="./images/MMIG-Bench/teaser2.png">
        <img class="moving-image" src="./images/MMIG-Bench/teaser2.png" alt="Moving Image 1" style="vertical-align:middle">
      </td>
      <td style="padding:20px;padding-top:20px;padding-bottom:20px;width:75%;vertical-align:middle">
        <p>
        <papertitle><a href="https://hanghuacs.github.io/MMIG-Bench/">MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models</a></papertitle><br>
        Hang Hua*, Ziyun Zeng*, <strong>Yizhi Song*</strong>, Yunlong Tang, Liu He, Daniel Aliaga, Wei Xiong, Jiebo Luo <br>
        <a href="https://arxiv.org/abs/2505.19415">PDF</a>
        /
        <a href="https://hanghuacs.github.io/MMIG-Bench/">Project Page</a>
        /
        <a href="https://github.com/hanghuacs/MMIG-Bench">Code</a>
        /
        <a href="https://huggingface.co/datasets/hhua2/MMIG-Bench">Data</a>
        <p><grey> We present MMIG-Bench, a comprehensive benchmark for evaluating multi-modal image generation models. MMIG-Bench unifies compositional evaluation across T2I and customized generation, introduces explainable aspect-level metrics, and offers a thorough analysis of SOTA diffusion, autoregressive, and API-based models (e.g., <strong>GPT-4o</strong>, <strong>Gemini</strong>). </grey></p>
      </td>
    </tr>

    <!-- MLLM -->
    <tr>
      <td class="moving-cell" style="padding:20px;padding-top:0;padding-bottom:0;width:25%;vertical-align:middle">
        <a href="./images/MLLM/MLLM_teaser.png">
        <img class="moving-image" src="./images/MLLM/MLLM_teaser.png" alt="Moving Image 1" style="vertical-align:middle">
      </td>
      <td style="padding:20px;padding-top:20px;padding-bottom:20px;width:75%;vertical-align:middle">
        <p>
        <papertitle><a href="https://arking1995.github.io/Advancing-VLMs">Advancing MLLMs by Large-Scale 3D Visual Instruction Dataset Generation</a></papertitle><br>
        Liu He, Xiao Zeng, <strong>Yizhi Song</strong>, Albert Y. C. Chen, Lu Xia, Shashwat Verma, Sankalp Dayal, Min Sun, Daniel Aliaga <br>
        <a href="">PDF (coming soon!)</a>
        /
        <a href="https://arking1995.github.io/Advancing-VLMs">Project Page</a>
        /
        Under review
        <p><grey> MLLMs struggle with accurately capturing camera-object relations, especially for object orientation, camera viewpoint, and camera shots. To address this, we propose a synthetic generation pipeline to create large-scale 3D visual instruction datasets. MLLMs fine-tuned on our dataset outperform commercial models by a large margin. </grey></p>
      </td>
    </tr>

    <!-- Artifacts -->
    <tr>
      <td class="moving-cell" style="padding:20px;padding-top:0;padding-bottom:0;width:25%;vertical-align:middle">
        <a href="./images/Artifacts/teaser3.png">
        <img class="moving-image" src="./images/Artifacts/teaser3.png" alt="Moving Image 1" style="vertical-align:middle">
      </td>
      <td style="padding:20px;padding-top:20px;padding-bottom:20px;width:75%;vertical-align:middle">
        <p>
        <papertitle><a href="https://song630.github.io/Refine-by-Align-Project-Page/">Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment</a></papertitle><br>
        <strong>Yizhi Song</strong>, Liu He, Zhifei Zhang, Soo Ye Kim, He Zhang, Wei Xiong, Zhe Lin, Brian L. Price, Scott Cohen, Jianming Zhang, Daniel Aliaga <br>
        <span style="color: red;">ICLR 2025</span> <br>
        <a href="https://arxiv.org/abs/2412.00306">PDF</a>
        /
        <a href="https://song630.github.io/Refine-by-Align-Project-Page/">Project Page</a>
        <p><grey> We introduce a new task: reference-guided refinement of generative artifacts. Given a synthesized image, a reference and a free-form mask marking the artifacts, the model automatically identifies the correspondence in the reference and extracts the localized feature, which is then used to fix the artifacts. </grey></p>
      </td>
    </tr>

    <!-- GroundingBooth -->
    <tr>
      <td class="moving-cell" style="padding:20px;padding-top:0;padding-bottom:0;width:25%;vertical-align:middle">
        <a href="./images/GroundingBooth/teaser.jpg">
        <img class="moving-image" src="./images/GroundingBooth/teaser.jpg" alt="Moving Image 1" style="vertical-align:middle">
      </td>
      <td style="padding:20px;padding-top:20px;padding-bottom:20px;width:75%;vertical-align:middle">
        <p>
        <papertitle><a href="https://groundingbooth.github.io/">GroundingBooth: Grounding Text-to-Image Customization</a></papertitle><br>
        Zhexiao Xiong, Wei Xiong, Jing Shi, He Zhang, <strong>Yizhi Song</strong>, Nathan Jacobs <br>
        <a href="https://arxiv.org/pdf/2409.08520">PDF</a>
        /
        <a href="https://groundingbooth.github.io/">Project Page</a>
        <p><grey> We introduce GroundingBooth, a framework that achieves zero-shot instance-level spatial grounding on both foreground subjects and background objects in the text-to-image customization task. </grey></p>
      </td>
    </tr>

    <!-- Kubrick -->
    <tr>
      <td class="moving-cell" style="padding:20px;padding-top:0;padding-bottom:0;width:25%;vertical-align:middle">
        <a href="./images/Kubrick/spiderman.mp4">
        <video width="100%" muted="" autoplay="autoplay" loop="">
          <source src="./images/Kubrick/spiderman.mp4" type="video/mp4" style="vertical-align:middle">
        </video></div>
      </td>
      <td style="padding:20px;padding-top:20px;padding-bottom:20px;width:75%;vertical-align:middle">
        <p>
        <papertitle><a href="https://arking1995.github.io/Kubrick/">Kubrick: Multimodal Agent Collaborations for Video Generation</a></papertitle><br>
        Liu He, <strong>Yizhi Song</strong>, Hejun Huang, Daniel Aliaga, Xin Zhou <br>
        <span style="color: red;">CVPR 2025 AI4CC Workshop </span> <br>
        <a href="https://www.arxiv.org/abs/2408.10453">PDF</a>
        /
        <a href="https://arking1995.github.io/Kubrick/">Project Page</a>
        <p><grey> We build the first multimodal agent-based video generation pipeline through 3D engine scripting. Given any text prompt, multimodal agents collaborate to produce detailed Blender scripts to generate video with plausible character and motion consistency in any length. </grey></p>
      </td>
    </tr>

    <!-- IMPRINT -->
    <tr>
      <td class="moving-cell" style="padding:20px;padding-top:0;padding-bottom:0;width:25%;vertical-align:middle">
        <a href="./images/IMPRINT/teaser.png">
        <img class="moving-image" src="./images/IMPRINT/teaser.png" alt="Moving Image 1" style="vertical-align:middle">
      </td>
      <td style="padding:20px;padding-top:20px;padding-bottom:20px;width:75%;vertical-align:middle">
        <p>
        <papertitle><a href="https://song630.github.io/IMPRINT-Project-Page/">IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation</a></papertitle><br>
        <strong>Yizhi Song</strong>, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian L. Price, Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, Daniel Aliaga <br>
        <span style="color: red;">CVPR 2024</span> <br>
        <a href="https://arxiv.org/pdf/2403.10701.pdf">PDF</a>
        /
        <a href="https://song630.github.io/IMPRINT-Project-Page/">Project Page</a>
        <p><grey> Our tuning-free model achieves advanced image composition with a decent <strong>identity preservation</strong>, automatic object viewpoint/pose adjustment, color and lighting harmonization, and shadow synthesis. All these effects are achieved in a single framework! </grey></p>

      </td>
    </tr>

    <!-- Thinking outside the bbox -->
    <tr>
      <td class="moving-cell" style="padding:20px;padding-top:0;padding-bottom:0;width:25%;vertical-align:middle">
        <a href="./images/Outside_the_bbox/teaser7.png">
        <img class="moving-image" src="./images/Outside_the_bbox/teaser7.png" alt="Moving Image 1" style="vertical-align:middle">
      </td>
      <td style="padding:20px;padding-top:20px;padding-bottom:20px;width:75%;vertical-align:middle">
        <p>
        <papertitle><a href="https://arxiv.org/abs/2409.04559">Thinking Outside the BBox: Unconstrained Generative Object Compositing</a></papertitle><br>
        Gemma Canet Tarr√©s, Zhe Lin, Zhifei Zhang, Jianming Zhang, <strong>Yizhi Song</strong>, Dan Ruta, Andrew Gilbert, John Collomosse, Soo Ye Kim <br>
        <span style="color: red;">ECCV 2024</span> <br>
        <a href="https://arxiv.org/abs/2409.04559">PDF</a>
        /
        <a href="">Project Page (coming soon!)</a>
        <p><grey> We introduce a novel task, <strong>unconstrained image compositing</strong>, where the generation is not bounded by the input mask and can even occur without one (thus supports <strong>automatic object placement</strong>). This allows the generation of realistic object effects (shadows and reflections) that go beyond the mask while preserving the surrounding background. </grey></p>

      </td>
    </tr>
    
    <!-- ObjectStitch -->
    <tr>
      <td class="moving-cell" style="padding:20px;padding-top:0;padding-bottom:0;width:25%;vertical-align:middle">
        <a href="./images/ObjectStitch/teaser_v6.jpg">
        <img class="moving-image" src="./images/ObjectStitch/teaser_v6.jpg" alt="Moving Image 1" style="vertical-align:middle">
      </td>
      <td style="padding:20px;padding-top:20px;padding-bottom:20px;width:75%;vertical-align:middle">
        <p>
        <papertitle><a href="https://arxiv.org/abs/2212.00932">ObjectStitch: Object Compositing With Diffusion Model</a></papertitle><br>
        <strong>Yizhi Song</strong>, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian L. Price, Jianming Zhang, Soo Ye Kim, Daniel Aliaga <br>
        <span style="color: red;">CVPR 2023</span> <br>
        <a href="https://song630.github.io/IMPRINT-Project-Page/">Project Page</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_ObjectStitch_Object_Compositing_With_Diffusion_Model_CVPR_2023_paper.pdf">Paper</a>
        /
        <a href="https://arxiv.org/abs/2212.00932">arXiv</a>
        /
        <strong><a href="https://x.com/_akhaliq/status/1599577845050793989">Reposted by AK</a></strong>
        /
        Appeared in <strong><a href="https://www.youtube.com/watch?v=HVxv5YdbdDg">Adobe Summit Sneak</a></strong>
        <p><grey> We define a novel task: <strong>generative image compositing</strong>, and present the <strong>first</strong> diffusion model-based framework, ObjectStitch, which can handle multiple aspects of compositing such as viewpoint, geometry, lighting and shadow together in a unified model. </grey></p>

      </td>
    </tr>
    <!-- End of project -->

    <!-- traffic sign -->
    <tr>
      <td class="moving-cell" style="padding:20px;padding-top:0;padding-bottom:0;width:25%;vertical-align:middle">
        <a href="./images/traffic_sign/traffic.png">
        <img class="moving-image" src="./images/traffic_sign/traffic.png" alt="Moving Image 1" style="vertical-align:middle">
      </td>
      <td style="padding:20px;padding-top:20px;padding-bottom:20px;width:75%;vertical-align:middle">
        <p>
        <papertitle><a href="https://link.springer.com/content/pdf/10.1007/s41095-019-0152-1.pdf">A Three-Stage Real-Time Detector for Traffic Signs in Large Panoramas</a></papertitle><br>
        <strong>Yizhi Song</strong>, Ruochen Fan, Sharon Huang, Zhe Zhu, Ruofeng Tong <br>
        <span style="color: red;">CVM 2019 (oral)</span> <br>
        <a href="https://link.springer.com/content/pdf/10.1007/s41095-019-0152-1.pdf">PDF</a>
        <p><grey> We propose a novel three-stage traffic sign detection framework which achieves state-ofthe-art detection accuracy in real-time. </grey></p>

      </td>
    </tr>

  </tbody>
</table>


<!-- <hr class="soft"> -->
<table
  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Work Experiences</h2>
        <p>Research scientist intern at <strong>Adobe</strong> (summer 2023)</p>
          <ul>
            <li>Project: object-centric image compositing.</li>
          </ul>
        <p>Research scientist intern at <strong>Adobe</strong> (summer 2022)</p>
          <ul>
            <li>Project: object-centric image compositing.</li>
          </ul>
        <p>Interim engineering intern at <strong>Qualcomm</strong> (summer 2021)</p>
          <ul>
            <li>Project: depth-aware image inpainting.</li>
          </ul>
      </td>
    </tr>
  </tbody>
</table>


<!-- <hr class="soft"> -->
<table
  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Academic Service</h2>
        <p>Reviewers: CVPR, ECCV (Outstanding Reviewer), ACMMM, NeurIPS, ICML</p>
      </td>
    </tr>
  </tbody>
</table>


<!-- <table
  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Patents</h2>
        <ul>
          <li><a href="https://patents.google.com/patent/US20250022099A1/en">Systems and Methods for Image Compositing</a> (2025). <strong>Yizhi Song</strong>, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim. US Patent: US20250022099A1.</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table> -->


<!-- <hr class="soft"> -->
<table
  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Teaching</h2>
        <ul>
          <li>CS 334 (Fundamentals Of Computer Graphics)</li>
          <li>CS 252 (Systems Programming)</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<!-- <hr class="soft"> -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
<tr>
  <td>
    <h2>Misc</h2>
    <p>I'm also an angler and a fan of musicals and original movie soundtracks.</p>
  </td>
</tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>


          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is built on <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>. Thanks for <a href="https://leonidk.com/">Leonid Keselman</a>'s Jekyll template.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
